{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdW8fzpanZTr"
      },
      "source": [
        "sentence_bn = ['আমরা আজ ল্যাব ক্লাস এ নিউরাল নেটওয়ার্ক প্রয়োগ করব', 'আমাদের প্রোগ্রামিং অনেক ভালো লাগে', 'আমরা সবাই এই কোর্সে ভালো মার্ক পেতে চাই']\n",
        "sentence_en = ['We do not know how to implement a neural network', 'We don\\'t like programking at all', 'We are in great doubt about our marks in this course']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3yP1_CeosWd",
        "outputId": "4e0b36e0-59f0-4ec8-9455-3e4d62a238d4"
      },
      "source": [
        "import nltk\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diXsU2vrpCug",
        "outputId": "86a1b03b-2dc2-4ca5-f692-6b3e41635297"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer()\n",
        "# vec.fit(sentence_bn)\n",
        "model = vec.fit_transform(sentence_en)\n",
        "\n",
        "vec.vocabulary_\n",
        "vocab_list = vec.get_feature_names()\n",
        "print(vocab_list)\n",
        "# print(model.toarray())\n",
        "count_list = model.toarray().sum(axis=0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['about', 'all', 'are', 'at', 'course', 'do', 'don', 'doubt', 'great', 'how', 'implement', 'in', 'know', 'like', 'marks', 'network', 'neural', 'not', 'our', 'programking', 'this', 'to', 'we']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kLWD_cYvWYk",
        "outputId": "7706c724-22eb-4e5e-ec6f-eb22aca55bc9"
      },
      "source": [
        "print(dict(zip(vocab_list,count_list)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'about': 1, 'all': 1, 'are': 1, 'at': 1, 'course': 1, 'do': 1, 'don': 1, 'doubt': 1, 'great': 1, 'how': 1, 'implement': 1, 'in': 2, 'know': 1, 'like': 1, 'marks': 1, 'network': 1, 'neural': 1, 'not': 1, 'our': 1, 'programking': 1, 'this': 1, 'to': 1, 'we': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPLjz4YCvu6Z",
        "outputId": "5483e115-622d-40c4-c00e-fa72f803335e"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "vec_new = CountVectorizer(encoding='utf-8', tokenizer=word_tokenize)\n",
        "vec_new.fit(sentence_bn)\n",
        "\n",
        "vocab_list_bn = vec_new.get_feature_names()\n",
        "print(vocab_list_bn)\n",
        "print(vec_new.vocabulary_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['অনেক', 'আজ', 'আমরা', 'আমাদের', 'এ', 'এই', 'করব', 'কোর্সে', 'ক্লাস', 'চাই', 'নিউরাল', 'নেটওয়ার্ক', 'পেতে', 'প্রোগ্রামিং', 'প্রয়োগ', 'ভালো', 'মার্ক', 'লাগে', 'ল্যাব', 'সবাই']\n",
            "{'আমরা': 2, 'আজ': 1, 'ল্যাব': 18, 'ক্লাস': 8, 'এ': 4, 'নিউরাল': 10, 'নেটওয়ার্ক': 11, 'প্রয়োগ': 14, 'করব': 6, 'আমাদের': 3, 'প্রোগ্রামিং': 13, 'অনেক': 0, 'ভালো': 15, 'লাগে': 17, 'সবাই': 19, 'এই': 5, 'কোর্সে': 7, 'মার্ক': 16, 'পেতে': 12, 'চাই': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aO98z51Hxeu1",
        "outputId": "53d6552b-ab4e-4bff-b31e-1b6aa79e9d36"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "tokenized_bn = []\n",
        "\n",
        "new_sentence = sentence_bn + ['এইখানে আমরা নতুন আরো একটি বাক্য রাখলাম']\n",
        "\n",
        "for sent in new_sentence:\n",
        "  tokenized_sent = word_tokenize(sent)\n",
        "  # print(tokenized_sent)\n",
        "  tokenized_bn.append(tokenized_sent)\n",
        "\n",
        "print(tokenized_bn)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['আমরা', 'আজ', 'ল্যাব', 'ক্লাস', 'এ', 'নিউরাল', 'নেটওয়ার্ক', 'প্রয়োগ', 'করব'], ['আমাদের', 'প্রোগ্রামিং', 'অনেক', 'ভালো', 'লাগে'], ['আমরা', 'সবাই', 'এই', 'কোর্সে', 'ভালো', 'মার্ক', 'পেতে', 'চাই'], ['এইখানে', 'আমরা', 'নতুন', 'আরো', 'একটি', 'বাক্য', 'রাখলাম']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXBzX-7-1J9a",
        "outputId": "c5b2856b-618a-4838-85ef-8dc85db41659"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "std_names = ['জিয়াদ', 'নয়ন', 'কিশোর', 'বিলাস', 'ইমতিয়াজুল', 'মনিরুল' , 'নয়ন']\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "name_labels = encoder.fit_transform(std_names)\n",
        "\n",
        "# dense representation\n",
        "print(name_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 1 4 0 5 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsBp-m1o1Ju0",
        "outputId": "8bc3a104-7288-468d-d49f-2c1db4135e48"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "name_labels = name_labels.reshape((7, 1))\n",
        "\n",
        "sparse_rep = encoder.fit_transform(name_labels).toarray()\n",
        "print(sparse_rep)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}
